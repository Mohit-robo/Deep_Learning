# Deep Learning Practice
### The main directory contains main files of the series of codes written for various Deep Learning terminologies.
The sub directories contain experimented codes for various activation functions -- sigmoid, tanh, weight initialization techniques -- He, Kaiming, etc and testing such other terminologies. 

### The basic code contains:

    2 Layers
    ReLU Activation function
    kaiming weight initialization
    

Throughout the Series we can experiment with various Weight Initialzation Techniques, Loss functions, Activation functions, Regulatization Techniques, Data Augmentation, Adding hidden layers,etc


The primary library around which all the codes will be written is **Numpy** and **Matplolib**
